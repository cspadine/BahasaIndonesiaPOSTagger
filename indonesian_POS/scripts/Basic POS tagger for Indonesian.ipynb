{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step one: Reformatting corpus data\n",
    "\n",
    "The data that I'll be using to train the taggers is in .conllu format, and the NLTK taggers require a different format, so the first step will be to change the training data into a usable format for the taggers. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sent_id = train-s1\n",
      "# text = Sembungan adalah sebuah desa yang terletak di kecamatan Kejajar, kabupaten Wonosobo, Jawa Tengah, Indonesia.\n",
      "1\tSembungan\tsembungan\tPROPN\tX--\t_\t4\tnsubj\t_\tMorphInd=^sembungan<x>_X--$\n",
      "2\tadalah\tadalah\tAUX\tO--\t_\t4\tcop\t_\tMorphInd=^adalah<o>_O--$\n",
      "3\tsebuah\tsebuah\tDET\tB--\tPronType=Ind\t4\tdet\t_\tMorphInd=^sebuah<b>_B--$\n",
      "4\tdesa\tdesa\tNOUN\tNSD\tNumber=Sing\t0\troot\t_\tMorphInd=^desa<n>_NSD$\n",
      "5\tyang\tyang\tPRON\tS--\tPronType=Rel\t6\tnsubj:pass\t_\tMorphInd=^yang<s>_S--$\n",
      "6\tterletak\tterletak\tVERB\tVSP\tNumber=Sing|Voice=Pass\t4\tacl\t_\tMorphInd=^ter+letak<n>_VSP$\n",
      "7\tdi\tdi\tADP\tR--\t_\t8\tcase\t_\tMorphInd=^di<r>_R--$\n",
      "8\tkecamatan\tkecamatan\tNOUN\tNSD\tNumber=Sing\t6\tobl\t_\tMorphInd=^ke+camat<n>+an_NSD$\n",
      "9\tKejajar\tkejajar\tPROPN\tX--\t_\t8\tflat\t_\tSpaceAfter=No|MorphInd=^kejajar<x>_X--$\n",
      "10\t,\t,\tPUNCT\tZ--\t_\t8\tpunct\t_\tMorphInd=^,<z>_Z--$\n",
      "11\tkabupaten\tkabupaten\tNOUN\tNSD\tNumber=Sing\t8\tappos\t_\tMorphInd=^kabupaten<n>_NSD$\n",
      "12\tWonosobo\twonosobo\tPROPN\tX--\t_\t11\tflat\t_\tSpaceAfter=No|MorphInd=^wonosobo<x>_X--$\n",
      "13\t,\t,\tPUNCT\tZ--\t_\t11\tpunct\t_\tMorphInd=^,<z>_Z--$\n",
      "14\tJawa\tjawa\tPROPN\tNSD\tNumber=Sing\t11\tappos\t_\tMorphInd=^jawa<n>_NSD$\n",
      "15\tTengah\ttengah\tPROPN\tNSD\tNumber=Sing\t14\tamod\t_\tSpaceAfter=No|MorphInd=^tengah<n>_NSD$\n",
      "16\t,\t,\tPUNCT\tZ--\t_\t14\tpunct\t_\tMorphInd=^,<z>_Z--$\n",
      "17\tIndonesia\tindonesia\tPROPN\tNSD\tNumber=Sing\t14\tappos\t_\tSpaceAfter=No|MorphInd=^indonesia<n>_NSD$\n",
      "18\t.\t.\tPUNCT\tZ--\t_\t4\tpunct\t_\tMorphInd=^.<z>_Z--$\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from conllu import parse\n",
    "\n",
    "file = open('../../UD_Indonesian-GSD/id_gsd-ud-train.conllu', 'r')\n",
    "f = file.read()\n",
    "file.close()\n",
    "\n",
    "print(f[:1393])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Fortunately, there is already a tool for this: Emil Stenstrom's conllu parser (https://github.com/EmilStenstrom/conllu) will turn the .conllu data into a nested dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TokenList<Sembungan, adalah, sebuah, desa, yang, terletak, di, kecamatan, Kejajar, ,, kabupaten, Wonosobo, ,, Jawa, Tengah, ,, Indonesia, .>, TokenList<Sebuah, serangan, pengayauan, biasanya, terjadi, di, ladang, atau, dengan, membakar, sebuah, rumah, dan, memenggal, semua, penghuninya, ketika, mereka, melarikan, diri, .>, TokenList<Perkembangan, ini, diikuti, oleh, helm, Brodie, yang, dipakai, tentara, Imperium, Britania, dan, AS, ,, dan, pada, tahun, 1916, oleh, Stahlhelm, Jerman, dengan, perbaikan, desain, yang, masih, dipakai, sampai, sekarang, .>, TokenList<Dari, jarak, dekat, ,, dua, kapal, perusak, ,, Sterett, dan, O, ', Bannon, menembakkan, beberapa, kali, salvo, ke, bangunan, atas, kapal, Hiei, ., Sebelum, melarikan, diri, ke, dalam, kegelapan, ,, keduanya, mungkin, sempat, menyarangkan, satu, atau, dua, buah, torpedo, ke, bagian, lambung, kapal, Hiei, hingga, makin, memperparah, kerusakan, .>, TokenList<Angka, harapan, hidup, adalah, 73, ,, 4, tahun, ,, yakni, di, bawah, rerata, Uni, Eropa, .>]\n"
     ]
    }
   ],
   "source": [
    "sentences = parse(f)\n",
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token([('id', 1), ('form', 'Sembungan'), ('lemma', 'sembungan'), ('upos', 'PROPN'), ('xpos', 'X--'), ('feats', None), ('head', 4), ('deprel', 'nsubj'), ('deps', None), ('misc', OrderedDict([('MorphInd', '^sembungan<x>_X--$')]))])\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Sembungan', 'PROPN'), ('adalah', 'AUX'), ('sebuah', 'DET'), ('desa', 'NOUN'), ('yang', 'PRON'), ('terletak', 'VERB'), ('di', 'ADP'), ('kecamatan', 'NOUN'), ('Kejajar', 'PROPN'), (',', 'PUNCT'), ('kabupaten', 'NOUN'), ('Wonosobo', 'PROPN'), (',', 'PUNCT'), ('Jawa', 'PROPN'), ('Tengah', 'PROPN'), (',', 'PUNCT'), ('Indonesia', 'PROPN'), ('.', 'PUNCT')], [('Sebuah', 'DET'), ('serangan', 'NOUN'), ('pengayauan', 'NOUN'), ('biasanya', 'ADV'), ('terjadi', 'VERB'), ('di', 'ADP'), ('ladang', 'NOUN'), ('atau', 'CCONJ'), ('dengan', 'ADP'), ('membakar', 'VERB'), ('sebuah', 'DET'), ('rumah', 'NOUN'), ('dan', 'CCONJ'), ('memenggal', 'VERB'), ('semua', 'DET'), ('penghuninya', 'NOUN'), ('ketika', 'SCONJ'), ('mereka', 'PRON'), ('melarikan', 'VERB'), ('diri', 'NOUN'), ('.', 'PUNCT')], [('Perkembangan', 'NOUN'), ('ini', 'DET'), ('diikuti', 'VERB'), ('oleh', 'ADP'), ('helm', 'NOUN'), ('Brodie', 'PROPN'), ('yang', 'PRON'), ('dipakai', 'VERB'), ('tentara', 'NOUN'), ('Imperium', 'PROPN'), ('Britania', 'PROPN'), ('dan', 'CCONJ'), ('AS', 'PROPN'), (',', 'PUNCT'), ('dan', 'CCONJ'), ('pada', 'ADP'), ('tahun', 'NOUN'), ('1916', 'NUM'), ('oleh', 'ADP'), ('Stahlhelm', 'PROPN'), ('Jerman', 'PROPN'), ('dengan', 'ADP'), ('perbaikan', 'NOUN'), ('desain', 'NOUN'), ('yang', 'PRON'), ('masih', 'ADV'), ('dipakai', 'VERB'), ('sampai', 'ADP'), ('sekarang', 'NOUN'), ('.', 'PUNCT')], [('Dari', 'ADP'), ('jarak', 'NOUN'), ('dekat', 'ADJ'), (',', 'PUNCT'), ('dua', 'NUM'), ('kapal', 'NOUN'), ('perusak', 'NOUN'), (',', 'PUNCT'), ('Sterett', 'PROPN'), ('dan', 'CCONJ'), ('O', 'PROPN'), (\"'\", 'PUNCT'), ('Bannon', 'PROPN'), ('menembakkan', 'VERB'), ('beberapa', 'DET'), ('kali', 'NOUN'), ('salvo', 'NOUN'), ('ke', 'ADP'), ('bangunan', 'NOUN'), ('atas', 'ADP'), ('kapal', 'NOUN'), ('Hiei', 'PROPN'), ('.', 'PUNCT'), ('Sebelum', 'SCONJ'), ('melarikan', 'VERB'), ('diri', 'NOUN'), ('ke', 'ADP'), ('dalam', 'ADP'), ('kegelapan', 'NOUN'), (',', 'PUNCT'), ('keduanya', 'NOUN'), ('mungkin', 'ADV'), ('sempat', 'ADV'), ('menyarangkan', 'VERB'), ('satu', 'NUM'), ('atau', 'CCONJ'), ('dua', 'NUM'), ('buah', 'NOUN'), ('torpedo', 'NOUN'), ('ke', 'ADP'), ('bagian', 'NOUN'), ('lambung', 'NOUN'), ('kapal', 'NOUN'), ('Hiei', 'PROPN'), ('hingga', 'ADP'), ('makin', 'ADV'), ('memperparah', 'VERB'), ('kerusakan', 'NOUN'), ('.', 'PUNCT')], [('Angka', 'PROPN'), ('harapan', 'NOUN'), ('hidup', 'NOUN'), ('adalah', 'AUX'), ('73', 'NUM'), (',', 'PUNCT'), ('4', 'NUM'), ('tahun', 'NOUN'), (',', 'PUNCT'), ('yakni', 'ADP'), ('di', 'ADP'), ('bawah', 'NOUN'), ('rerata', 'NOUN'), ('Uni', 'PROPN'), ('Eropa', 'PROPN'), ('.', 'PUNCT')]]\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tagged_sentence = []\n",
    "    for token in sentence:\n",
    "        tagged_sentence.append((token['form'],token['upos']))\n",
    "    training_data.append(tagged_sentence)\n",
    "    \n",
    "print(training_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = open('../../UD_Indonesian-GSD/id_gsd-ud-test.conllu', 'r')\n",
    "test_f = test_file.read()\n",
    "test_file.close()\n",
    "\n",
    "test_sentences = parse(test_f)\n",
    "\n",
    "test_data = []\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    tagged_sentence = []\n",
    "    for token in sentence:\n",
    "        tagged_sentence.append((token['form'],token['upos']))\n",
    "    test_data.append(tagged_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram tagger accuracy: 0.8404074702886248\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NOUN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(training_data, backoff=default_tagger)\n",
    "unigram_tagger_accuracy = unigram_tagger.evaluate(test_data)\n",
    "\n",
    "print('Unigram tagger accuracy: ' + str(unigram_tagger_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram tagger accuracy: 0.8400679117147708\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(training_data, backoff=unigram_tagger)\n",
    "bigram_tagger_accuracy = bigram_tagger.evaluate(test_data)\n",
    "\n",
    "print('Bigram tagger accuracy: ' + str(bigram_tagger_accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram tagger accuracy: 0.8089983022071308\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unigram_tagger = nltk.UnigramTagger(training_data, backoff=default_tagger)\n",
    "\n",
    "bigram_tagger_accuracy = bigram_tagger.evaluate(test_data)\n",
    "print('Bigram tagger accuracy: ' + str(bigram_tagger_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
